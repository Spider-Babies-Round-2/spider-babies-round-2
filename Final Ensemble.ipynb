{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Ensemble Learner\n",
    "\n",
    "## What's happening here?\n",
    "\n",
    "You'll see that the first three sections creates learners.   \n",
    "This does not mean that the learners created here are the ones that go in the ensemble. I believe our Random Forest is the only one which is created in advance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest made up of DTrees (learner #1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mltools as ml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.genfromtxt('data/X_train.txt', delimiter=',')\n",
    "Y = np.genfromtxt('data/Y_train.txt', delimiter=',')\n",
    "\n",
    "X,Y = ml.shuffleData(X,Y)\n",
    "\n",
    "Xtr, Xva, Ytr, Yva = ml.splitData(X,Y,0.7)\n",
    "\n",
    "X_numeric = Xtr[:,:41]\n",
    "X_discrete = Xtr[:,41:69]\n",
    "X_binary = Xtr[:,69:-1]\n",
    "\n",
    "Xtr_kaggle = np.genfromtxt('data/X_train.txt', delimiter=',')\n",
    "Ytr_kaggle = np.genfromtxt('data/Y_train.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggedTree(ml.base.classifier):\n",
    "    def __init__(self, learners):\n",
    "        \"\"\"Constructs a BaggedTree class with a set of learners. \"\"\"\n",
    "        self.learners = learners\n",
    "    \n",
    "    def predictSoft(self, X):\n",
    "        \"\"\"Predicts the probabilities with each bagged learner and average over the results. \"\"\"\n",
    "        n_bags = len(self.learners)\n",
    "        preds = [self.learners[l].predictSoft(X) for l in range(n_bags)]\n",
    "        return np.mean(preds, axis=0)\n",
    "\n",
    "bootstrap_sample_size = 50\n",
    "\n",
    "m,n = Xtr.shape\n",
    "\n",
    "bag_numbers = np.array([5,10,25,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of numeric learners = 33\n",
      "# of discrete learners = 10\n",
      "# of binary learners = 5\n",
      "---- Total number of bags for this run: 48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Xtr_kaggle = Xtr_kaggle[:,:41]\n",
    "\n",
    "Xtr_kaggle,Ytr_kaggle = ml.shuffleData(Xtr_kaggle,Ytr_kaggle)\n",
    "\n",
    "#final_num_bags = 50\n",
    "#final_classifiers = [None]*final_num_bags\n",
    "\n",
    "num_bags = 50\n",
    "\n",
    "num_numeric_learners = int(round(num_bags / 1.5))\n",
    "num_discrete_learners = int(round(num_bags / 5))\n",
    "num_binary_learners = int(round(num_bags / 10))\n",
    "    \n",
    "# update num bags based on number of learners \n",
    "# (only slight deviation based on the algorithm and rounding)\n",
    "num_bags = num_numeric_learners + num_discrete_learners + num_binary_learners\n",
    "\n",
    "print(f\"# of numeric learners = {num_numeric_learners}\")\n",
    "print(f\"# of discrete learners = {num_discrete_learners}\")\n",
    "print(f\"# of binary learners = {num_binary_learners}\")\n",
    "print(f\"---- Total number of bags for this run: {num_bags}\")\n",
    "\n",
    "\n",
    "classifiers = [None]*num_bags\n",
    "\n",
    "# keep track of which number classifier we are on\n",
    "classifiers_index = 0\n",
    "\n",
    "for i in range(num_numeric_learners):\n",
    "    #print(\"classifier index\", classifiers_index)\n",
    "    Xi,Yi = ml.bootstrapData(Xtr, Ytr)\n",
    "\n",
    "    # insert classifier into list\n",
    "    classifiers[classifiers_index] = ml.dtree.treeClassify(Xi, Yi, minParent=400, minLeaf=100, maxDepth=50)\n",
    "    classifiers_index += 1\n",
    "\n",
    "for i in range(num_discrete_learners):\n",
    "    #print(\"classifier index\", classifiers_index)\n",
    "    Xi,Yi = ml.bootstrapData(Xtr, Ytr)\n",
    "\n",
    "    # insert classifier into list\n",
    "    classifiers[classifiers_index] = ml.dtree.treeClassify(Xi, Yi, minParent=300, minLeaf=100, maxDepth=10)\n",
    "    classifiers_index += 1\n",
    "\n",
    "for i in range(num_binary_learners):\n",
    "    #print(\"classifier index\", classifiers_index)\n",
    "    Xi,Yi = ml.bootstrapData(Xtr, Ytr)\n",
    "\n",
    "    # insert classifier into list\n",
    "    classifiers[classifiers_index] = ml.dtree.treeClassify(Xi, Yi, minParent=16, minLeaf=50, maxDepth=10)\n",
    "    classifiers_index += 1\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "kaggle_bagged_tree = BaggedTree(classifiers)\n",
    "kaggle_bagged_tree.classes = np.unique(Ytr_kaggle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Learner (Learner #2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mltools as ml\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = np.genfromtxt('data/X_train.txt', delimiter=',')\n",
    "Y = np.genfromtxt('data/Y_train.txt', delimiter=',')\n",
    "\n",
    "np.random.seed(0)\n",
    "X,Y = ml.shuffleData(X,Y)\n",
    "X_numeric = X[:,:41]\n",
    "X_categorical = X[:,41:69]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "Xtr, Xva, Ytr, Yva = ml.splitData(X_categorical,Y,0.7)\n",
    "Xtr_scaled = scaler.fit_transform(Xtr, Ytr)\n",
    "Xva_scaled = scaler.fit_transform(Xva,Yva)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=410, weights='distance')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_knn = KNeighborsClassifier(n_neighbors=410, weights='distance')\n",
    "sklearn_knn.fit(Xtr_scaled, Ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Learner (Learner #3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier as mlpc\n",
    "from sklearn.preprocessing import StandardScaler,QuantileTransformer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "\n",
    "import mltools as ml\n",
    "X = np.genfromtxt('data/X_train.txt', delimiter=',')\n",
    "Y = np.genfromtxt('data/Y_train.txt', delimiter=',')\n",
    "np.random.seed(0)\n",
    "\n",
    "X,Y = ml.shuffleData(X,Y)\n",
    "X = X[:,:41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "transformer = QuantileTransformer()\n",
    "\n",
    "Xtr, Xva, Ytr, Yva = ml.splitData(X,Y,0.7)\n",
    "Xtr_scaled = scaler.fit_transform(Xtr, Ytr)\n",
    "Xva_scaled = scaler.fit_transform(Xva,Yva)\n",
    "Xtr_transformed = transformer.fit_transform(Xtr)\n",
    "Xva_transformed = transformer.fit_transform(Xva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.05, hidden_layer_sizes=(5, 5, 10),\n",
       "              learning_rate='adaptive', max_iter=10000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = mlpc(max_iter=10000, alpha=0.05, hidden_layer_sizes=(5,5,10), activation='tanh', learning_rate= 'adaptive')\n",
    "learner.fit(Xtr_transformed,Ytr) #the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte = np.genfromtxt('data/X_test.txt', delimiter=',')\n",
    "Xte_transformed = transformer.fit_transform(Xte[:,:41])\n",
    "Yte_hat = np.vstack((np.arange(Xte_transformed.shape[0]), learner.predict(Xte_transformed))).T\n",
    "\n",
    "np.savetxt('Y_submit_neural_network.txt', Yte_hat,'%d, %.2f',comments='', header='Id,Predicted', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Ensemble class\n",
    "In this case, we are using the BaggedTree class to implement a Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "\n",
    "class BaggedTree(ml.base.classifier):\n",
    "    def __init__(self, learners):\n",
    "        \"\"\"Constructs a BaggedTree class with a set of learners. \"\"\"\n",
    "        self.learners = learners\n",
    "    \n",
    "    def predictSoft(self, X):\n",
    "        \"\"\"Predicts the probabilities with each bagged learner and average over the results. \"\"\"\n",
    "        n_bags = len(self.learners)\n",
    "        preds = []\n",
    "        for l in range(n_bags):\n",
    "            prediction = 0\n",
    "            learner = self.learners[l]\n",
    "            \n",
    "            if isinstance(learner,ClassifierMixin):\n",
    "                prediction = learner.predict_proba(X)\n",
    "            else:\n",
    "                prediction = learner.predictSoft(X)\n",
    "                \n",
    "            preds.append(prediction)\n",
    "        return np.mean(preds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mltools as ml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.genfromtxt('data/X_train.txt', delimiter=',')\n",
    "Y = np.genfromtxt('data/Y_train.txt', delimiter=',')\n",
    "\n",
    "X,Y = ml.shuffleData(X,Y)\n",
    "\n",
    "Xtr, Xva, Ytr, Yva = ml.splitData(X,Y,0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data\n",
    "\n",
    "I'm giving us options to use three types of data:  \n",
    "- Raw (`Xtr`/`Xva`)  \n",
    "- Scaled (`Xtr_scaled`,`Xva_scaled`)  \n",
    "- Transformed (`Xtr_transformed`/`Xva_transformed`)\n",
    "\n",
    "**KNOW THIS:** Whatever preprocessing actions we do to the training data MUST BE DONE ON THE FINAL `Xte` ON WHICH OUR FINAL ENSEMBLE MAKES PREDICTIONS FOR KAGGLE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "transformer = QuantileTransformer()\n",
    "\n",
    "Xtr, Xva, Ytr, Yva = ml.splitData(X,Y,0.7)\n",
    "Xtr_scaled = scaler.fit_transform(Xtr, Ytr)\n",
    "Xva_scaled = scaler.fit_transform(Xva,Yva)\n",
    "Xtr_transformed = transformer.fit_transform(Xtr)\n",
    "Xva_transformed = transformer.fit_transform(Xva)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the list of classifiers for our Final Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of numeric learners = 33\n",
      "# of discrete learners = 5\n",
      "# of binary learners = 10\n",
      "---- Total number of bags for this run: 48\n"
     ]
    }
   ],
   "source": [
    "bootstrap_sample_size = 50\n",
    "\n",
    "\n",
    "num_bags = 50\n",
    "\n",
    "num_random_forest_learners = int(round(num_bags / 1.5))\n",
    "num_knn_learners = int(round(num_bags / 10))\n",
    "num_neural_network_learners = int(round(num_bags / 5))\n",
    "    \n",
    "# update num bags based on number of learners \n",
    "# (only slight deviation based on the algorithm and rounding)\n",
    "num_bags = num_random_forest_learners + num_knn_learners + num_neural_network_learners\n",
    "\n",
    "print(f\"# of numeric learners = {num_random_forest_learners}\")\n",
    "print(f\"# of discrete learners = {num_knn_learners}\")\n",
    "print(f\"# of binary learners = {num_neural_network_learners}\")\n",
    "print(f\"---- Total number of bags for this run: {num_bags}\")\n",
    "\n",
    "\n",
    "final_classifiers = [None]*num_bags\n",
    "\n",
    "# keep track of which number classifier we are on\n",
    "classifiers_index = 0\n",
    "\n",
    "\n",
    "# Populate classifier list with our random forest learners\n",
    "for i in range(num_random_forest_learners):\n",
    "    Xi,Yi = ml.bootstrapData(Xtr, Ytr)\n",
    "\n",
    "    # insert classifier into list\n",
    "    \n",
    "    # WE CAN'T TRAIN OUR RANDOM FOREST AGAIN\n",
    "    final_classifiers[classifiers_index] = kaggle_bagged_tree\n",
    "    classifiers_index += 1\n",
    "\n",
    "for i in range(num_knn_learners):\n",
    "    Xi,Yi = ml.bootstrapData(Xtr, Ytr)\n",
    "\n",
    "    # insert classifier into list\n",
    "    knn = KNeighborsClassifier(n_neighbors=410, weights='distance')\n",
    "    knn.fit(Xi, Yi)\n",
    "    final_classifiers[classifiers_index] = knn\n",
    "    classifiers_index += 1\n",
    "\n",
    "for i in range(num_neural_network_learners):\n",
    "    Xi,Yi = ml.bootstrapData(Xtr, Ytr)\n",
    "\n",
    "    # insert classifier into list\n",
    "    neural_network_learner = mlpc(max_iter=10000, alpha=0.05, hidden_layer_sizes=(5,5,10), activation='tanh', learning_rate= 'adaptive')\n",
    "    neural_network_learner.fit(Xi,Yi) #the training\n",
    "    final_classifiers[classifiers_index] = neural_network_learner\n",
    "    classifiers_index += 1\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "final_ensemble = BaggedTree(final_classifiers)\n",
    "final_ensemble.classes = np.unique(Ytr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc 0.7862609338266497\n"
     ]
    }
   ],
   "source": [
    "auc = final_ensemble.auc(Xva, Yva)\n",
    "\n",
    "print(\"auc\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
